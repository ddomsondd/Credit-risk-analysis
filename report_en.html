
<!DOCTYPE html>
<html>
<head>    
    <title>Atlas Bootcamp</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="container">

    <div class="header">
      <h1>Creditworthiness Analysis</h1>
    </div>
    
    <div class="main">
      <h2>Introduction</h2>
      <p>This report was created using libraries such as:
        <ul>
          <li>Numpy</li>
          <li>Pandas</li>
          <li>Matplotlib</li>
          <li>Scikit-learn</li>
          <li>XGBoost</li>
          <li>Seaborn</li>
          <li>Imbalanced-Learn</li>
        </ul>
      </p>
      <br>
    
      <h2>Exploration and Data Preparation</h2>
      <p>I started working on the report by exploring the dataset and performing data analysis. I used the <span style="font-style: italic;">data_check()</span> function, which displays column header names, unique values in the columns, and the count of NaN values:</p>
    <pre><code>
          def data_check():
            data = pd.read_csv('data_atlas.csv')
            headers = data.columns.to_list()
            print(headers)

            #checking columns names and unique values 
            df = pd.DataFrame(data)
            
            for column in df.columns:
                print(f"Liczba wystąpień wartości w kolumnie '{column}':")
                print(df[column].value_counts())
                print(df[column].unique())
                print(df[column].isna().sum())
                print()
        </code></pre>
        <br>
        <br>
        The table below shows the first few rows of the dataset:
      </p>
   <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>age</th>
      <th>income</th>
      <th>children</th>
      <th>credit_history</th>
      <th>overdue_payments</th>
      <th>active_loans</th>
      <th>years_in_job</th>
      <th>employment_type</th>
      <th>owns_property</th>
      <th>assets_value</th>
      <th>other_loans</th>
      <th>education</th>
      <th>city</th>
      <th>marital_status</th>
      <th>support_indicator</th>
      <th>credit_risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>44</td>
      <td>15689 złoty</td>
      <td>brak</td>
      <td>dobra historia</td>
      <td>brak opóźnień</td>
      <td>2</td>
      <td>9</td>
      <td>samozatrudnienie</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1</td>
      <td>wyższe</td>
      <td>małe</td>
      <td>żonaty/zamężna</td>
      <td>0.178131</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>38</td>
      <td>18906 złoty</td>
      <td>4 dzieci</td>
      <td>brak historii</td>
      <td>brak opóźnień</td>
      <td>0</td>
      <td>1</td>
      <td>stała</td>
      <td>tak</td>
      <td>62965 złoty</td>
      <td>0</td>
      <td>średnie</td>
      <td>średnie</td>
      <td>kawaler/panna</td>
      <td>0.370480</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>46</td>
      <td>16338 złoty</td>
      <td>2 dzieci</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2</td>
      <td>4</td>
      <td>brak</td>
      <td>tak</td>
      <td>124967 złoty</td>
      <td>0</td>
      <td>podstawowe</td>
      <td>duże</td>
      <td>żonaty/zamężna</td>
      <td>0.712334</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>55</td>
      <td>23276 złoty</td>
      <td>3 dzieci</td>
      <td>dobra historia</td>
      <td>opóźnienia</td>
      <td>2</td>
      <td>10</td>
      <td>stała</td>
      <td>tak</td>
      <td>52147 złoty</td>
      <td>1</td>
      <td>średnie</td>
      <td>małe</td>
      <td>kawaler/panna</td>
      <td>0.665050</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>37</td>
      <td>40000 złoty</td>
      <td>1 dzieci</td>
      <td>brak historii</td>
      <td>NaN</td>
      <td>1</td>
      <td>9</td>
      <td>określona</td>
      <td>nie</td>
      <td>33957 złoty</td>
      <td>1</td>
      <td>wyższe</td>
      <td>małe</td>
      <td>kawaler/panna</td>
      <td>0.607151</td>
      <td>0</td>
    </tr>
  </tbody>
</table>            
<p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Table 1. Display of the first few rows of the dataset.</p>

<br>
<p>Here are the selected summary numerical data:</p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>income</th>
      <th>children</th>
      <th>active_loans</th>
      <th>years_in_job</th>
      <th>assets_value</th>
      <th>other_loans</th>
      <th>credit_risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>39.527000</td>
      <td>18878.897000</td>
      <td>0.983600</td>
      <td>1.497900</td>
      <td>9.524300</td>
      <td>64279.536200</td>
      <td>0.390600</td>
      <td>0.032500</td>
    </tr>
    <tr>
      <th>std</th>
      <td>9.874899</td>
      <td>12831.656498</td>
      <td>1.235105</td>
      <td>1.223824</td>
      <td>4.869803</td>
      <td>70789.054207</td>
      <td>0.487909</td>
      <td>0.177333</td>
    </tr>
    <tr>
      <th>min</th>
      <td>18.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.000000</td>
      <td>10390.750000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>6.000000</td>
      <td>17170.500000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>39.000000</td>
      <td>18786.500000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>9.000000</td>
      <td>45976.500000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>46.000000</td>
      <td>28167.500000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>13.000000</td>
      <td>87158.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>70.000000</td>
      <td>40000.000000</td>
      <td>5.000000</td>
      <td>8.000000</td>
      <td>30.000000</td>
      <td>500000.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Table 2. Summary data for the table.</p>
<br>
<p>
    Some columns, such as 'Unnamed: 0', 'support_indicator', do not provide any significant data, so they can be removed right away. 
    Data in certain columns also needs to be transformed to be easily interpreted by the program, for example, by removing unnecessary strings like ("children", "zlotych"), replacing NaN cells with 'unknown' or zero values, which I will address later. 
    I also noticed that the 'credit_history' column should contain values like 'no history', 'good history', and 'bad history', however, the last one is missing from the dataset, and instead, there are NaN values, which I replaced with a specific value.
    I performed this in the <span style="font-style: italic;">data_preparation()</span> function in the <span style="font-style: italic;">data_preparation.py</span> file, which looks as follows:
</p>
 <pre><code>
            def data_preparation():
                data = pd.read_csv('data_atlas.csv')

                data = data.drop(['Unnamed: 0', 'support_indicator '], axis=1)

                data['credit_history'] = data['credit_history'].fillna('zła historia')
                data['overdue_payments'] = data['overdue_payments'].fillna('unknown')
                data['owns_property'] = data['owns_property'].fillna('unknown')
                data['children'] = data['children'].str.replace(' dzieci', '').replace('brak', '0').astype(int)
                data['income'] = data['income'].fillna('0 złoty')
                data['income'] = data['income'].str.replace(' złoty', '').astype(int)
                data['assets_value'] = data['assets_value'].fillna('0 złoty')
                data['assets_value'] = data['assets_value'].str.replace(' złoty', '').astype(int)

                return data
          </code></pre>
          <br>
          <p>This is how the data looks after its initial processing:</p>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>income</th>
      <th>children</th>
      <th>credit_history</th>
      <th>overdue_payments</th>
      <th>active_loans</th>
      <th>years_in_job</th>
      <th>employment_type</th>
      <th>owns_property</th>
      <th>assets_value</th>
      <th>other_loans</th>
      <th>education</th>
      <th>city</th>
      <th>marital_status</th>
      <th>credit_risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>44</td>
      <td>15689</td>
      <td>0</td>
      <td>dobra historia</td>
      <td>brak opóźnień</td>
      <td>2</td>
      <td>9</td>
      <td>samozatrudnienie</td>
      <td>unknown</td>
      <td>0</td>
      <td>1</td>
      <td>wyższe</td>
      <td>małe</td>
      <td>żonaty/zamężna</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38</td>
      <td>18906</td>
      <td>4</td>
      <td>brak historii</td>
      <td>brak opóźnień</td>
      <td>0</td>
      <td>1</td>
      <td>stała</td>
      <td>tak</td>
      <td>62965</td>
      <td>0</td>
      <td>średnie</td>
      <td>średnie</td>
      <td>kawaler/panna</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>46</td>
      <td>16338</td>
      <td>2</td>
      <td>zła historia</td>
      <td>unknown</td>
      <td>2</td>
      <td>4</td>
      <td>brak</td>
      <td>tak</td>
      <td>124967</td>
      <td>0</td>
      <td>podstawowe</td>
      <td>duże</td>
      <td>żonaty/zamężna</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>55</td>
      <td>23276</td>
      <td>3</td>
      <td>dobra historia</td>
      <td>opóźnienia</td>
      <td>2</td>
      <td>10</td>
      <td>stała</td>
      <td>tak</td>
      <td>52147</td>
      <td>1</td>
      <td>średnie</td>
      <td>małe</td>
      <td>kawaler/panna</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>37</td>
      <td>40000</td>
      <td>1</td>
      <td>brak historii</td>
      <td>unknown</td>
      <td>1</td>
      <td>9</td>
      <td>określona</td>
      <td>nie</td>
      <td>33957</td>
      <td>1</td>
      <td>wyższe</td>
      <td>małe</td>
      <td>kawaler/panna</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Table 3. Display of the first few rows of the dataset after data processing.</p>
<br>
Next, I replace the cells in the 'income' and 'assets_value' columns with zero values with more representative data. I have three options for this:
<ul>
  <li>Replacing zero values with the mean</li>
  <li>Replacing zero values with values determined using the KNN algorithm</li>
  <li>Replacing zero values with values determined by linear regression</li>
</ul>
Function for replacing zero values with the mean:<br><br>

          <pre><code>
            def change_zero_values_to_mean():
                data = data_preparation()
                mean_income = int(data[data['income'] != 0]['income'].mean())
                #print(mean_income)
                data.loc[data['income'] == 0, 'income'] = mean_income

                mean_assets_value = int(data[data['assets_value'] != 0]['assets_value'].mean())
                #print(mean_assets_value)
                data.loc[data['assets_value'] == 0, 'assets_value'] = mean_assets_value
                income_zero_count = data[data['income'] == 0].shape[0]
                assets_value_zero_count = data[data['assets_value'] == 0].shape[0]
                #print(f'INCOME 0: {income_zero_count}')
                #print(f'ASSETS_VALUE 0: {assets_value_zero_count}')
                
                return data
          </code></pre>
          <br><br>
          Function for replacing zero values with values determined by KNN:<br><br>
          <pre><code>            
            def change_zero_values_with_knn():
                data = data_preparation()
                imputer = KNNImputer(n_neighbors=5)
                data['income'] = data['income'].replace(0, np.nan)
                data['income'] = imputer.fit_transform(data[['income']])

                data['assets_value'] = data['assets_value'].replace(0, np.nan)
                data['assets_value'] = imputer.fit_transform(data[['assets_value']])

                return data
          </code></pre>
          <br><br>
          Functions for replacing zero values with values determined by linear regression:<br><br>
          <pre><code>
          def fill_zero_values(data, target_column, model):
              train_data = data[data[target_column] != 0]
              test_data = data[data[target_column] == 0]

              X_train = train_data.drop([target_column], axis=1)
              y_train = train_data[target_column]

              cross_val = cross_val_score(model, X_train, y_train, cv=5, scoring='r2') 
              y_pred = cross_val_predict(model, X_train, y_train, cv=5)

              r2 = r2_score(y_train, y_pred)
              mse = np.sqrt(mean_squared_error(y_train, y_pred))
              mae = mean_absolute_error(y_train, y_pred)

              model.fit(X_train, y_train)

              X_test = test_data.drop([target_column], axis=1)
              predictions = model.predict(X_test)
              data.loc[data[target_column] == 0, target_column] = predictions

              results = {
                  'R2 Score:': r2,
                  'MSE': mse,
                  'MAE': mae
              }
              return data


          def data_modification():
              one_hot_data = data_preparation_one_hot('regression')

              model_income = LinearRegression()
              one_hot_data = fill_zero_values(one_hot_data, 'income', model_income)

              model_assets = LinearRegression()
              one_hot_data = fill_zero_values(one_hot_data, 'assets_value', model_assets)
              
              """
              df_income = pd.DataFrame(results_income)
              print(df_income)

              df_assets = pd.DataFrame(results_assets)
              print(df_assets)
              """
              
              return one_hot_data
          </code></pre>
          <br><br>
          The next step was to transform the columns into one-hot encoded columns. I do this using the <span style="font-style: italic;">data_preparation_one_hot()</span> function, which takes one argument specifying the method to use for replacing zero values. After appropriately replacing the zero values, I call the <span style="font-style: italic;">get_dummies()</span> method from the Pandas library, specifying which columns I want to transform into one-hot encoded data.<br><br>
          <pre><code>
          def data_preparation_one_hot(operation):
              if operation == 'mean':
                  data = change_zero_values_to_mean()
              elif operation == 'knn':
                  data = change_zero_values_with_knn()
              else:
                  data = data_preparation()
              one_hot_data = pd.get_dummies(data, columns = ['credit_history', 'overdue_payments', 
                'employment_type', 'owns_property', 'education', 'city', 'marital_status'], dtype=int)

              return one_hot_data
          </code></pre> 
          <br>
          <p>All data processing functions are contained in the <span style="font-style: italic;">data_preparation.py</span> and <span style="font-style: italic;">regression.py</span> files. The prepared data can then be subjected to further analysis.</p>
          <br>
          
          <h2>Sample Charts for the Dataset</h2>
          <p>All the charts were created in the <span style="font-style: italic;">data_visualistion.py</span> file.</p>
          <div class="img_class">
            <div class="box">
              <div style="flex-direction: column;">
                <img src="plots/visualisations/age_income_en.png" arc="" style="margin-right: 30px;">
                <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Figure 1.</p>
              </div>
              <div style="width: 55%">
                The chart shows the average annual income in relation to age. <br>As can be seen, the chart fluctuates significantly, especially for age groups from 20 to 30 years and from 50 to 70 years. For the age range of 30 to 50 years, the average annual income is around 23-24 thousand PLN. The largest fluctuations in average annual income occur after the age of 60, where income changes from 20 thousand to even 27 thousand PLN.<br>
                The chart was created using the following code: <br><br>
          
            <pre style="margin: 0; width: 100%;"><code style="font-size: medium;">
      plt.figure(figsize=(8,6))
      plt.plot(mean_income_by_age['age'], mean_income_by_age['income'], color='purple', label='Średni roczny przychód', linewidth=2)
      plt.title('Average annual income depending on age')
      plt.xlabel('Age')
      plt.ylabel('Annual income (zł)')
      plt.legend(title='Legend', labels=['Average annual income'])
      #plt.show()
      plt.savefig(f'plots/visualisations/age_income_en.png')
            </code></pre>
          </div>
        </div>

        <br>

        <div class="box">
          <div style="flex-direction: column;">
            <img src="plots/visualisations/children_credit_history_en.png" arc="" style="margin-right: 30px;">
            <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Figure 2.</p>
          </div>
          <div style="width: 55%">
            The chart shows a comparison of the number of children with credit history. As seen, the most loans are taken by people without children. With each additional child, the number of loans decreases. However, it is worth noting that the number of people without a credit history who do not have children is also the highest, and by a significant margin, which indicates that the majority of samples in the dataset are childless individuals.<br>
            The chart was created using the following code:<br><br>          
            <pre style="margin: 0; width: 100%;"><code style="font-size: medium;">
      credit_history_by_children = (
          data.groupby('children')['credit_history']
          .value_counts(normalize=True)
          .unstack()
          .reset_index()
      )

      plt.figure(figsize=(10, 6))
      sns.countplot(data=data, x='children', hue='credit_history', palette='rocket')
      plt.title('Number of people with different credit histories divided by the number of children')
      plt.xlabel('Number of children')
      plt.ylabel('Number of people')
      plt.legend(title='Credit history', labels=['No history', 'Good history', 'Bad history'])
      plt.savefig(f'plots/visualisations/children_credit_history_en.png')
            </code></pre>
          </div>
        </div>

        <br>

        <div class="box">
          <div style="flex-direction: column;">
            <img src="plots/visualisations/income_en.png" arc="" style="margin-right: 30px;">
            <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Figure 3.</p>
            </div>
            <div style="width: 65%">
              The chart shows a comparison of average annual income relative to the number of people receiving each specific salary.<br>Immediately noticeable is the value in the range of approximately 20-25 thousand PLN per year. The number of people with this annual income is the highest because this value was assigned to cells with NaN values. This is the average value of all incomes in that column. Another interesting point is that a suddenly large number of people earn an annual salary of 40 thousand PLN, showing a sharp increase. In third place are salaries in the range of 15-20 thousand PLN per year.<br>
              The chart was created using the following code:<br><br>            
            <pre style="margin: 0; width: 100%;"><code style="font-size: medium;">
      plt.figure(figsize=(8, 6))
      sns.histplot(data['income'], bins=10, kde=True, color='#f6bcba')
      plt.title('Distribution of annual income')
      plt.xlabel('Annual income (zł)')
      plt.ylabel('Number of people')
      plt.savefig(f'plots/visualisations/income_en.png')
            </code></pre>
          </div>
        </div>
        
        <br>

        <div class="box">
          <div style="flex-direction: column;">
            <img src="plots/visualisations/employment_type_credit_risk_en.png" arc="" style="margin-right: 30px;">
            <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Figure 4.</p>
          </div>
          <div style="width: 55%">
            The chart shows employment types in relation to financial risk.<br>As seen, the lowest credit risk is for people with permanent jobs, and in second and third place are self-employed individuals or those on fixed-term contracts. Interestingly, the highest credit risk is also for people with permanent jobs, rather than those without work, which could be due to the significantly smaller sample size of unemployed individuals. This chart also highlights the imbalance between classes representing high and low credit risk.<br>
            The chart was created using the following code:<br><br>          
            <pre style="margin: 0; width: 100%;"><code style="font-size: medium;">
      plt.figure(figsize=(8, 6))
      sns.countplot(data=data, x='employment_type', hue='credit_risk', palette='rocket')
      plt.title('Employment type to credit risk')
      plt.xlabel('Employment type')
      plt.ylabel('Number of people')
      plt.legend(title='Credit risk', labels=['Low', 'High'])
      plt.savefig(f'plots/visualisations/employment_type_credit_risk_en.png')
            </code></pre>
          </div>
        </div>
      </div>
      <br>

      <h2>Data Correlation Matrix</h2>
      <p>I also created a correlation matrix for the numerical data using the code below, located in the <span style="font-style: italic;">data_visualisation.py</span> file.</p>
      <div class="img_inline">
        <div style="flex-direction: column;">
          <img src="plots/visualisations/corr_matrix_en.png" alt="Correlation Matrix" style="width: 500px;">
          <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Figure 5. Correlation Matrix.</p>
        </div>
      </div>
      <p>You can observe that the data is weakly correlated with each other, with the 'active_loans' column standing out, which has a correlation of about 0.31 with the 'credit_risk' column. This indicates that it may have the greatest influence on the prediction of this variable compared to the other features in this dataset. However, this matrix shows a lack of clear dependencies between the variables. This suggests an opportunity for further data transformation, but that is a topic for later.</p>
      <br>
      
      <h2>Classification Models</h2>
      <p>When creating the classification models, I assumed that the target value would be credit risk (the 'credit_risk' column).<br>I compared five classification models. The models were created in the <span style="font-style: italic;">classification.py</span> file.
        <ul>
          <li>Decision Tree Classifier</li>
          <li>Random Forest Classifier</li>
          <li>Support Vector Classifier</li>
          <li>Gaussian Naive Bayes</li> 
          <li>XGBoost Classifier</li>
        </ul>
      
      In the <span style="font-style: italic;">execute_classification()</span> function, I split the dataset into independent input data X, from which the model will learn predictions, and dependent data y, which should be predicted – in this case, the y set consists of the <span style="font-style: italic;">'credit_risk'</span> column, as I want to predict high or low credit risk. I then check the distribution of samples in this class, and I get 9675 samples for class 0 (low credit risk) and 325 for class 1 (high credit risk). As can be seen, the classes are highly imbalanced. This is presented in the chart below:
      <br><br>
      <div class="img_inline">
        <div style="flex-direction: column;">
          <img src="plots/more_infos/sample_distribution_en.png" alt="Class Distribution" style="width: 500px;">
          <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Figure 6. Distribution of predicted class values.</p>
        </div>
      </div>
      <br>
      To address this issue, I balance the classes using the SMOTE method, which adds samples to the underrepresented class based on the existing dataset. After performing this operation, both classes have an equal number of samples, which is 9675, as shown in the following chart:
      <br><br>
      <div class="img_inline">
        <div style="flex-direction: column;">
          <img src="plots/more_infos/sample_distribution_smote_en.png" alt="Class Distribution after SMOTE" style="width: 500px;">
          <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Figure 7. Distribution of predicted class values after applying the SMOTE method.</p>
        </div>
      </div>
      <br>Next, I define which models I want to create, and in a loop, I call the <span style="font-style: italic;">create_classification_model()</span> method, which creates these models and returns accuracy, sensitivity, specificity, F1, and Cohen Kappa metrics. I also call the method that generates confusion matrices, but more on that later in the report.<br>
      <br>
      
        <pre><code>
          def execute_classification(one_hot_data):
              X = one_hot_data.drop('credit_risk', axis=1)
              y = one_hot_data['credit_risk']

              #checking samples distribution
              count_class = y.value_counts()
              #print(count_class)
              #english version
              plt.bar(count_class.index, count_class.values)
              plt.xlabel('Class')
              plt.ylabel('Count')
              plt.title('Class Distribution')
              plt.xticks(count_class.index, ['Class 0', 'Class 1'])
              plt.savefig(f'plots/sample_distribution_en.png', bbox_inches='tight')

              #polish version
              plt.bar(count_class.index, count_class.values)
              plt.xlabel('Klasa')
              plt.ylabel('Liczba próbek')
              plt.title('Rozkład klas')
              plt.xticks(count_class.index, ['Klasa 0', 'Klasa 1'])
              plt.savefig(f'plots/sample_distribution_pl.png', bbox_inches='tight')


              #class balancing
              smote=SMOTE(sampling_strategy='minority') 
              X,y=smote.fit_resample(X,y)
              #print(y.value_counts())


              models = [DecisionTreeClassifier(), RandomForestClassifier(), SVC(), GaussianNB(), XGBClassifier()]
              models_names = ['Decision Tree Classifier', 'Random Forest Classifier', 'Support Vector Classifier', 
                                'Gausian Naive Bayes', 'XGBoost Classifier']
              results = []

              for model, model_name in zip(models, models_names):
                  accuracy, recall, specificity, f1, kappa = create_classification_model(model, X, y, model_name)
                  results.append({
                      'Model': model_name,
                      'Accuracy': accuracy,
                      'Recall': recall,
                      'Specificity': specificity,
                      'F1 Score': f1,
                      'Cohen Kappa': kappa,
                  })

              return results
        </code></pre>
        <br><br>
        The function that creates models and metrics:<br><br>
        <pre><code>
          def create_classification_model(model, X, y, model_name):
              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

              model.fit(X_train, y_train)
              y_pred = model.predict(X_test)
              
              accuracy = round(accuracy_score(y_test, y_pred), 4)
              recall = round(recall_score(y_test, y_pred), 4)
              specificity = round(specificity_score(y_test, y_pred), 4)
              f1 = round(f1_score(y_test, y_pred), 4)
              kappa = round(cohen_kappa_score(y_test, y_pred), 4)
              
              if model_name == 'Random Forest Classifier':
                  headers = X.columns.to_list()

                  importances = model.feature_importances_
                  indices = np.argsort(importances)[::-1]

                  plt.figure(figsize=(10, 6))
                  plt.title("Feature Importances")
                  plt.bar(range(X.shape[1]), importances[indices], color='purple')
                  plt.xticks(range(X.shape[1]), X, rotation=90)
                  plt.xlabel("Importance")
                  plt.ylabel("Features")
                  plt.savefig(f'plots/features_importance_rfc_en.png', bbox_inches='tight')

                  plt.figure(figsize=(10, 6))
                  plt.title("Ważność cech")
                  plt.bar(range(X.shape[1]), importances[indices], color='purple')
                  plt.xticks(range(X.shape[1]), X, rotation=90)
                  plt.xlabel("Ważność")
                  plt.ylabel("Cecha")
                  plt.savefig(f'plots/features_importance_rfc_pl.png', bbox_inches='tight')

              
              plot_confmat(y_test, y_pred, model_name)

              return accuracy, recall, specificity, f1, kappa
        </code></pre>
        <br><br>


        <h3>Feature Importance</h3>
        <p>It is worth noting that when the model is a Random Forest, I create a feature importance chart that defines the degree of influence each feature has on the prediction outcome. The chart is as follows:</p>
        
        <div class="img_inline">
          <div style="flex-direction: column;">
            <img src="plots/more_infos/features_importance_rfc_en.png" alt="Feature Importance" style="width: 750px;">
            <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Figure 8. Feature Importance.</p>
          </div>
        </div>
    
        <p>However, I need to work on this further, as I am unsure if it is truly reliable, since these features are simply listed in the order they appear in the data table. Therefore, it is difficult to determine whether their importance actually follows this sequence.</p>
        <br>
        
        <h3>Accuracy Metrics for Individual Models</h3>
        <p>Moving on to the model metrics - as shown in the table below, the best-fitting models are the Random Forest and XGBoost. The Decision Tree model performs slightly worse, but still very well. These three models achieve good metrics for accuracy, sensitivity, specificity, as well as more precise metrics such as F1 Score and Cohen Kappa. Therefore, it can be concluded that they predict the target variable well. On the other hand, the metrics for the Support Vector Machine and Naive Bayes Classifier are worse. In these cases, the models perform poorly in terms of prediction, and improvements could be explored in the future.</p>
        <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model</th>
      <th>Accuracy</th>
      <th>Recall</th>
      <th>Specificity</th>
      <th>F1 Score</th>
      <th>Cohen Kappa</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Decision Tree Classifier</td>
      <td>0.9943</td>
      <td>0.9943</td>
      <td>0.9943</td>
      <td>0.9943</td>
      <td>0.9886</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Random Forest Classifier</td>
      <td>0.9961</td>
      <td>0.9923</td>
      <td>1.0000</td>
      <td>0.9961</td>
      <td>0.9922</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Support Vector Classifier</td>
      <td>0.5209</td>
      <td>0.4253</td>
      <td>0.6172</td>
      <td>0.4712</td>
      <td>0.0425</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Gausian Naive Bayes</td>
      <td>0.7827</td>
      <td>0.9119</td>
      <td>0.6525</td>
      <td>0.8081</td>
      <td>0.5650</td>
    </tr>
    <tr>
      <th>4</th>
      <td>XGBoost Classifier</td>
      <td>0.9974</td>
      <td>0.9949</td>
      <td>1.0000</td>
      <td>0.9974</td>
      <td>0.9948</td>
    </tr>
  </tbody>
</table>
      <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">Table 4. Presentation of the quality measures of the fitting of these models.</p>
      <br>

      <h3>Confusion Matrices for Individual Models</h3>
      <p>The confusion matrices were created using the <span style="font-style: italic;">plot_confmat()</span> method:</p>
      <pre><code>
        def plot_confmat(y_test, y_pred, title):
            cm = confusion_matrix(y_test, y_pred)

            cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm)
            cm_disp.plot(cmap = plt.cm.Purples)
            plt.title(f"Confusion matrix for {title} model")
            plt.savefig(f'plots/models/plot_class_{title}_en.png', bbox_inches='tight')

            cm_disp.plot(cmap = plt.cm.Purples)
            plt.title(f"Macierz pomyłek dla modelu {title}")
            plt.xlabel("Wartość przewidziana")
            plt.ylabel("Wartość prawdziwa")
            plt.savefig(f'plots/models/plot_class_{title}_pl.png', bbox_inches='tight')
      </code></pre>
      <br>
      <p>The results are as follows:</p>
      <div class="img_inline">
        <div style="flex-direction: column;">
          <img src="plots/models/plot_class_Decision Tree Classifier_en.png" arc="Confusion matrix for Decision Tree Classifier">
          <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">1. Confusion matrix for Decision Tree Model.</p>
        </div>
        <div style="flex-direction: column;">
          <img src="plots/models/plot_class_Random Forest Classifier_en.png" arc="Confusion matrix for Random Forest Classifier">
          <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">2. Confusion matrix for Random Forest Model.</p>
        </div>
        <div style="flex-direction: column;">
          <img src="plots/models/plot_class_Support Vector Classifier_en.png" arc="Confusion matrix for Support Vector Machine Classifier">
          <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">3. Confusion matrix for Support Vector Machine Model.</p>
        </div>
        <div style="flex-direction: column;">
          <img src="plots/models/plot_class_Gausian Naive Bayes_en.png" arc="Confusion matrix for Gaussian Naive Bayes Classifier">
          <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">4. Confusion matrix for Gaussian Naive Bayes Model.</p>
        </div>
        <div style="flex-direction: column;">
          <img src="plots/models/plot_class_XGBoost Classifier_en.png" arc="Confusion matrix for XGBoost Classifier">
          <p style="font-style: italic; font-size: 14px; text-align: center; margin-top: 2px;">5. Confusion matrix for XGBoost Model.</p>
        </div>
      </div>
      <p>The confusion matrices for the Random Forest, XGBoost, and Decision Tree models indicate high accuracy in classifying examples into the true-positive and true-negative sets, which is a good result. There are minor deviations, especially in the Decision Tree, where 18 samples were classified as false-positive and 9 as false-negative. In the case of Random Forest, there were 16 misclassified samples, and in the XGBoost model, 10.
        <br><br>The situation is significantly worse for the Support Vector Machine and Naive Bayes Classifier. In these cases, very poor prediction results were achieved.</p>

      <br>
      <h2>SUMMARY</h2>
      <p>I successfully created models predicting the occurrence or absence of credit risk. The best-performing models are Random Forest, XGBoost, and Decision Tree. Support Vector Machine and Naive Bayes Classifier, on the other hand, achieve poor prediction results, which creates room for improvement in future iterations of the project. However, the performance of all models has significantly improved, as confusion matrices in the previous version of the report indicated almost 99% alignment with the true-positive set. These models, however, were not representative due to the lack of balanced classes. Now, the confusion matrices of the best models show alignment of predictions with the true-positive and true-negative sets and a nearly 50%-50% balance, which is the desired outcome.
          <br><br>While working on this project, I implemented three methods for filling NaN values in numerical columns: filling with the mean, regression, and the k-nearest neighbors algorithm. In the final version of the report, I used the first option because it is the most optimal and there are no noticeable differences in the performance of the models using any of the methods. I tested all three approaches, and the results are very similar. Nevertheless, I am still not entirely convinced whether these are the best solutions, as the samples are heavily influenced by a single value, the mean. It might be worth considering whether there is a better method for replacing NaN values in these columns.
          <br><br>A future area of development could include data transformation to increase correlation and improve feature importance.
          <br><br>The report has also been improved regarding the number of samples. In the earlier version, I removed a significant part of the dataset using the dropna() function. This time, I replaced the missing values appropriately and balanced the prediction dataset `y` so that the classes are split 50%-50%. As a result, I obtained an even larger dataset.
          <br><br>Another area for future development is data visualization, as it may be worth considering more charts representing the data, tailored to the report's topic.
          <br><br>The report could also be extended with probabilistic plots and quantile analysis.
      </p>

      </p>
    </div>
    <div class="footer"></div>
  </div>
</body>
</html> 
